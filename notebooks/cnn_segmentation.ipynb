{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "# Local Imports\n",
    "from src.models.CNN import CNNModel\n",
    "from src.data.voxelize import voxelize\n",
    "from src.data.utils import read_voxel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "       \n",
    "# Paths\n",
    "LABELED_DATA = os.path.abspath(\"../data/labeled/\")\n",
    "VOXELIZED_DATA = os.path.abspath(\"../data/voxelized/amazon.h5py\")\n",
    "VOXEL_DIR = os.path.abspath(\"../data/tiles/\")\n",
    "# Model Variables \n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class VoxelDataset(Dataset):\n",
    "    def __init__(self, voxel_dir, transform=None, target_transform=None):\n",
    "        self.voxel_dir = voxel_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.voxel_files = [f for f in listdir(voxel_dir) if isfile(join(voxel_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voxel_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        voxel_path = os.path.join(self.voxel_dir, self.voxel_files[idx])\n",
    "        #voxels, labels = read_voxel_data(voxel_path)\n",
    "        #voxels = torch.from_numpy(voxels).float()\n",
    "        #labels = torch.from_numpy(labels).long()\n",
    "        with h5py.File(voxel_path, \"r\") as hf:\n",
    "            arr = hf['default'][:]\n",
    "        voxels = arr[:,:,:,0:7]\n",
    "        labels = arr[:,:,:,-1]\n",
    "        if self.transform:\n",
    "            voxels = self.transform(voxels)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "        return voxels, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train = VoxelDataset(VOXEL_DIR)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_size, padding=1, n_features=7):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels, out_channels, filter_size, padding=padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(p=0.2),\n",
    "            nn.Conv3d(out_channels, out_channels, filter_size, padding=padding),\n",
    "                     )\n",
    "    def forward(self, x):\n",
    "        o = self.conv(x)\n",
    "        return o\n",
    "\n",
    "    \n",
    "class MinimalSegmenter(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(MinimalSegmenter, self).__init__()\n",
    "        self.drop = nn.Dropout3d(p=0.2)\n",
    "        self.mp = nn.MaxPool3d((2,2,2))\n",
    "        self.e1 = ConvBlock(in_channels, 32, 3)\n",
    "        self.e2 = ConvBlock(32, 64, 3)\n",
    "        self.e3 = ConvBlock(64,128, 3)\n",
    "        self.d1 = nn.ConvTranspose3d(128, 64, 3, stride=2, padding=1)\n",
    "        self.dc1 = ConvBlock(128, 64, 3)\n",
    "        self.d2 = nn.ConvTranspose3d(64, 32, 3, stride=2, padding=1)\n",
    "        self.dc2 = ConvBlock(64, 32,3)\n",
    "        self.out = nn.Conv3d(32,n_classes,1)\n",
    "    def forward(self, x):\n",
    "        o = self.e1(x)\n",
    "        s1 = o\n",
    "        print(\"s1: \",s1.size())\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        o = self.e2(o)\n",
    "        s2 = o\n",
    "        print(\"s2: \",s2.size())\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        o = self.e3(o)\n",
    "        \n",
    "        o = self.d1(o)\n",
    "        print(\"d1: \", o.size())\n",
    "        o = torch.cat((o, s2))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc1(o)\n",
    "        \n",
    "        o = self.d2(o)\n",
    "        print(\"d2: \",o.size())\n",
    "        o = torch.cat((o, s2))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc2(o)\n",
    "        \n",
    "        o = self.out(o)\n",
    "        o = F.log_softmax(o, dim=3)\n",
    "        return o\n",
    "    \n",
    "# Contains bugs, WIP.\n",
    "class ForestSegmenter(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(ForestSegmenter, self).__init__()\n",
    "        # dropout layer\n",
    "        self.drop = nn.Dropout3d(p=0.2)\n",
    "        \n",
    "        # pooling layers\n",
    "        self.mp = nn.MaxPool3d((2,2,2))\n",
    "\n",
    "        # encoder part\n",
    "        self.e1 = ConvBlock(in_channels, 32, 3)\n",
    "        self.e2 = ConvBlock(32,64,3)\n",
    "        self.e3 = ConvBlock(64,128,3)\n",
    "        self.e4 = ConvBlock(128,256,3)\n",
    "        self.e5 = ConvBlock(256,512,3)\n",
    "        \n",
    "        # decoder part\n",
    "        self.d1 = nn.ConvTranspose3d(512,256,3, stride=2,  padding=1)\n",
    "        self.dc1 = ConvBlock(512, 256, 3)\n",
    "        self.d2 = nn.ConvTranspose3d(256,128,3, stride=2, padding=1)\n",
    "        self.dc2 = ConvBlock(256,128,3)\n",
    "        self.d3 = nn.ConvTranspose3d(128,64,3, stride=2,  padding=1)\n",
    "        self.dc3 = ConvBlock(128,64,3)\n",
    "        self.d4 = nn.ConvTranspose3d(64,32,3, stride=2,  padding=1)\n",
    "        self.dc4 = ConvBlock(64,32,3)\n",
    "        \n",
    "        # output\n",
    "        self.out = nn.Conv3d(32,n_classes,1)\n",
    "    def forward(self, x):\n",
    "        o = self.e1(x)\n",
    "        s1 = o\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        print(o.size())\n",
    "\n",
    "        o = self.e2(o)\n",
    "        s2 = o\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        print(o.size())\n",
    "        \n",
    "        o = self.e3(o)\n",
    "        s3 = o\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        print(o.size())\n",
    "        \n",
    "        o = self.e4(o)\n",
    "        s4 = o\n",
    "        o = self.mp(o)\n",
    "        o = self.drop(o)\n",
    "        print(o.size())\n",
    "        \n",
    "        o = self.e5(o)\n",
    "        print(o.size())\n",
    "\n",
    "  \n",
    "        # decode\n",
    "        print(\"decoding\")\n",
    "        o = self.d1(o)\n",
    "        print(o.size())\n",
    "\n",
    "        o = torch.cat((o, s4))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc1(o)\n",
    "        print(o.size())\n",
    "        \n",
    "        o = self.d2(o)\n",
    "        print(o.size())\n",
    "\n",
    "        o = torch.cat((o, s3))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc2(o)\n",
    "        print(o.size())\n",
    "        \n",
    "        o = self.d3(o)\n",
    "        o = torch.cat((o, s2))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc3(o)\n",
    "        \n",
    "        o = self.d4(o)\n",
    "        o = torch.cat((o, s1))\n",
    "        o = self.drop(o)\n",
    "        o = self.dc3(o)\n",
    "        \n",
    "        o = self.out(o)\n",
    "        o = F.log_softmax(o, dim=3)\n",
    "        return o\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    " \n",
    "model = MinimalSegmenter(7,2).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:  torch.Size([1, 32, 100, 100, 370])\n",
      "s2:  torch.Size([1, 64, 50, 50, 185])\n",
      "d1:  torch.Size([1, 64, 49, 49, 183])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 49 but got size 50 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27662/2882682286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27662/3473180973.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d1: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 49 but got size 50 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Training \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.view(x.size(0), x.size(4), x.size(1), x.size(2), x.size(3)).to(device)\n",
    "        y = y.view(y.size(0), y.size(1), y.size(2), y.size(3)).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = loss_func(y_, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del x,y,y_\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70f9338631b1e814274bb6be12ff2af50260dd3a60751a7a596f31e11ca463b8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
