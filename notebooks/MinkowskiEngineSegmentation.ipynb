{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauri/anaconda3/lib/python3.8/site-packages/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import MinkowskiEngine as ME\n",
    "import MinkowskiEngine.MinkowskiFunctional as MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "       \n",
    "# Paths\n",
    "DATA_PATH = os.path.abspath(\"../data/h5/\")\n",
    "\n",
    "# Model Variables \n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "L2 = 0.005\n",
    "MOMENTUM = 0.9\n",
    "QUANTIZATION_SIZE=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device=\"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, data_dir,  quantization_size=0.005):\n",
    "        self.data_dir = data_dir\n",
    "        self.quantization_size=quantization_size\n",
    "        self.data_files = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_path = os.path.join(self.data_dir, self.data_files[idx])\n",
    "        with h5py.File(data_path, \"r\") as hf:\n",
    "            coords = hf['coords'][:]\n",
    "            features = hf['feats'][:]\n",
    "            labels = np.int_(hf['labels'][:])\n",
    "        discrete_coords, unique_feats, unique_labels = ME.utils.sparse_quantize(\n",
    "            coordinates=coords,\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            quantization_size=self.quantization_size)\n",
    "        return discrete_coords, unique_feats, unique_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "train = SparseDataset(DATA_PATH, quantization_size=QUANTIZATION_SIZE)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, collate_fn=ME.utils.batch_sparse_collate, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(ME.MinkowskiNetwork):\n",
    "    def __init__(self, in_channels, out_channels, D=3):\n",
    "        super(ConvBlock, self).__init__(D)\n",
    "        self.block = nn.Sequential(\n",
    "            ME.MinkowskiBatchNorm(in_channels),\n",
    "            ME.MinkowskiReLU(),\n",
    "            ME.MinkowskiConvolution(in_channels, out_channels, kernel_size=3, stride=1, dimension=D),\n",
    "            ME.MinkowskiBatchNorm(out_channels),\n",
    "            ME.MinkowskiReLU(),\n",
    "            ME.MinkowskiConvolution(out_channels, out_channels, kernel_size=3, stride=1, dimension=D)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MiniSeg(ME.MinkowskiNetwork):\n",
    "    def __init__(self, in_channels, n_classes, D=3):\n",
    "        super(MiniSeg, self).__init__(D)\n",
    "        self.pool = ME.MinkowskiMaxPooling(2, dimension=D)\n",
    "        self.SM = ME.MinkowskiSoftmax()\n",
    "        self.c1 = ConvBlock(in_channels, 32, D)\n",
    "        self.c2 = ConvBlock(32, 64, D)\n",
    "        self.c3 = ConvBlock(64, 128, D)\n",
    "        \n",
    "        self.t1 = ME.MinkowskiConvolutionTranspose(128, 64, kernel_size=3, stride=1, dimension=D)\n",
    "        self.c4 = ConvBlock(128, 64, D)\n",
    "        self.t2 = ME.MinkowskiConvolutionTranspose(64, 32,kernel_size=3, stride=1, dimension=D)\n",
    "        self.c5 = ConvBlock(64, 32,D)\n",
    "        \n",
    "        self.out = ME.MinkowskiConvolution(32, n_classes, kernel_size=1, dimension=D)\n",
    "    def forward(self, x):\n",
    "        cat1 = self.c1(x)\n",
    "        o = self.pool(cat1)\n",
    "        cat2 = self.c2(o)\n",
    "        o = self.pool(cat2)\n",
    "        \n",
    "        o = self.c3(o)\n",
    "        \n",
    "        o = self.t1(o)\n",
    "        o = ME.cat(cat2, o)\n",
    "        o = self.c4(o)\n",
    "        o = self.t2(o)\n",
    "        o = ME.cat(cat1, o)\n",
    "        o = self.c5(o)\n",
    "        \n",
    "        o = self.out(o)\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    " \n",
    "model = MiniSeg(2, 2, D=3).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=L2, momentum=MOMENTUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 0: loss 18.721812188625336\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 1: loss 18.480863392353058\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 2: loss 18.199890196323395\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 3: loss 17.84956920146942\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 4: loss 17.378940045833588\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 5: loss 16.7643284201622\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 6: loss 16.022866249084473\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 7: loss 15.23841255903244\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 8: loss 14.537706971168518\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 9: loss 14.006826788187027\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 10: loss 13.6455679833889\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 11: loss 13.402512282133102\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 12: loss 13.228166550397873\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n",
      "Processing tile  24\n",
      "Processing tile  25\n",
      "Processing tile  26\n",
      "Epoch 13: loss 13.092429369688034\n",
      "Processing tile  0\n",
      "Processing tile  1\n",
      "Processing tile  2\n",
      "Processing tile  3\n",
      "Processing tile  4\n",
      "Processing tile  5\n",
      "Processing tile  6\n",
      "Processing tile  7\n",
      "Processing tile  8\n",
      "Processing tile  9\n",
      "Processing tile  10\n",
      "Processing tile  11\n",
      "Processing tile  12\n",
      "Processing tile  13\n",
      "Processing tile  14\n",
      "Processing tile  15\n",
      "Processing tile  16\n",
      "Processing tile  17\n",
      "Processing tile  18\n",
      "Processing tile  19\n",
      "Processing tile  20\n",
      "Processing tile  21\n",
      "Processing tile  22\n",
      "Processing tile  23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9005/3084805416.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/MinkowskiEngine/MinkowskiConvolution.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_out_feat)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbw_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minkowski_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ConvolutionBackward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_out_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         grad_in_feat, grad_kernel = bw_fn(\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mgrad_out_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, data in enumerate(train_loader): \n",
    "        print(\"Processing tile \",i)\n",
    "        coords, feats, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        #print(coords)\n",
    "        #print(feats)\n",
    "        #print(labels)\n",
    "        y_ = model(ME.SparseTensor(feats.float(), coords, device=device))\n",
    "        loss = loss_func(y_.F.squeeze(), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        del coords, feats, labels,y_\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {epoch}: loss {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70f9338631b1e814274bb6be12ff2af50260dd3a60751a7a596f31e11ca463b8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
